{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eacf8e2-6065-412e-88f3-5eae97596d78",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2f4e4-24a8-4250-956d-c11fce5451ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.ndimage import rotate\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal\n",
    "import scipy.sparse as sp\n",
    "from scipy.linalg import block_diag\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import time\n",
    "from torch_scatter import scatter_mean, scatter_max, scatter_add\n",
    "from torch_geometric.utils import remove_self_loops,add_self_loops\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "import torch_scatter\n",
    "import inspect\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daec86a-35da-4071-9ac1-175351a862c0",
   "metadata": {},
   "source": [
    "## Step 2: Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1977de-1b13-4f66-aa81-e5a80570816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uniform(size, tensor):\n",
    "    stdv = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "def glorot(tensor):\n",
    "    stdv = math.sqrt(6.0 / (tensor.size(0) + tensor.size(1)))\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "\n",
    "def ones(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(1)\n",
    "\n",
    "\n",
    "def reset(nn):\n",
    "    def _reset(item):\n",
    "        if hasattr(item, 'reset_parameters'):\n",
    "            item.reset_parameters()\n",
    "\n",
    "    if nn is not None:\n",
    "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
    "            for item in nn.children():\n",
    "                _reset(item)\n",
    "        else:\n",
    "            _reset(nn)\n",
    "\n",
    "\n",
    "def scatter_(name, src, index, dim_size=None):\n",
    "    r\"\"\"Aggregates all values from the :attr:`src` tensor at the indices\n",
    "    specified in the :attr:`index` tensor along the first dimension.\n",
    "    If multiple indices reference the same location, their contributions\n",
    "    are aggregated according to :attr:`name` (either :obj:`\"add\"`,\n",
    "    :obj:`\"mean\"` or :obj:`\"max\"`).\n",
    "    Args:\n",
    "        name (string): The aggregation to use (:obj:`\"add\"`, :obj:`\"mean\"`,\n",
    "            :obj:`\"max\"`).\n",
    "        src (Tensor): The source tensor.\n",
    "        index (LongTensor): The indices of elements to scatter.\n",
    "        dim_size (int, optional): Automatically create output tensor with size\n",
    "            :attr:`dim_size` in the first dimension. If set to :attr:`None`, a\n",
    "            minimal sized output tensor is returned. (default: :obj:`None`)\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "\n",
    "    assert name in ['add', 'mean', 'max']\n",
    "\n",
    "    op = getattr(torch_scatter, 'scatter_{}'.format(name))\n",
    "    fill_value = -1e38 if name is 'max' else 0   \n",
    "    out = op(src, index, 0, None, dim_size)\n",
    "    if isinstance(out, tuple):\n",
    "        out = out[0]\n",
    "\n",
    "    if name is 'max':\n",
    "        out[out == fill_value] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "size = 10\n",
    "tensor = torch.empty(size)\n",
    "uniform(size, tensor)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "        \n",
    "# Define a tensor with size (3, 4)\n",
    "tensor = torch.empty(3, 4)\n",
    "\n",
    "# Initialize the tensor using the glorot function\n",
    "glorot(tensor)\n",
    "\n",
    "# Print the initialized tensor\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8391053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Define a simple neural network\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the initial parameters of the neural network\n",
    "print(\"Initial parameters:\")\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param)\n",
    "\n",
    "\n",
    "reset(net)\n",
    "\n",
    "# Print the reset parameters of the neural network\n",
    "print(\"\\nReset parameters:\")\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ef593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    " # Define the source tensor\n",
    "src = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Define the index tensor\n",
    "index = torch.tensor([0, 1, 0])\n",
    "\n",
    "# Call the scatter_() function with the \"add\" aggregation\n",
    "out = scatter_('add', src, index)\n",
    "\n",
    "# Print the output tensor\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acbe5f-739e-4099-988b-b25e63d9d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "    r\"\"\"Base class for creating message passing layers\n",
    "    .. math::\n",
    "        \\mathbf{x}_i^{\\prime} = \\gamma_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i,\n",
    "        \\square_{j \\in \\mathcal{N}(i)} \\, \\phi_{\\mathbf{\\Theta}}\n",
    "        \\left(\\mathbf{x}_i, \\mathbf{x}_j,\\mathbf{e}_{i,j}\\right) \\right),\n",
    "    where :math:`\\square` denotes a differentiable, permutation invariant\n",
    "    function, *e.g.*, sum, mean or max, and :math:`\\gamma_{\\mathbf{\\Theta}}`\n",
    "    and :math:`\\phi_{\\mathbf{\\Theta}}` denote differentiable functions such as\n",
    "    MLPs.\n",
    "    See `here <https://rusty1s.github.io/pytorch_geometric/build/html/notes/\n",
    "    create_gnn.html>`__ for the accompanying tutorial.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aggr='add'):\n",
    "        super(MessagePassing, self).__init__()\n",
    "\n",
    "\n",
    "        message_signature = inspect.signature(self.message)\n",
    "        # Extract the names of parameters excluding the first one (self for methods)\n",
    "        self.message_args = [param.name for param in message_signature.parameters.values()][0:]\n",
    "        update_signature = inspect.signature(self.update)\n",
    "        # Extract the names of parameters excluding the first one (self for methods)\n",
    "        self.update_args = [param.name for param in update_signature.parameters.values()][1:]\n",
    "\n",
    "    def propagate(self, aggr, edge_index, **kwargs):\n",
    "        r\"\"\"The initial call to start propagating messages.\n",
    "        Takes in an aggregation scheme (:obj:`\"add\"`, :obj:`\"mean\"` or\n",
    "        :obj:`\"max\"`), the edge indices, and all additional data which is\n",
    "        needed to construct messages and to update node embeddings.\"\"\"\n",
    "\n",
    "        assert aggr in ['add', 'mean', 'max']\n",
    "        kwargs['edge_index'] = edge_index\n",
    "\n",
    "        size = None\n",
    "        message_args = []\n",
    "        for arg in self.message_args:\n",
    "            if arg[-2:] == '_i':\n",
    "                tmp = kwargs[arg[:-2]]\n",
    "                size = tmp.size(0)\n",
    "                message_args.append(tmp[edge_index[0]])\n",
    "            elif arg[-2:] == '_j':\n",
    "                tmp = kwargs[arg[:-2]]\n",
    "                size = tmp.size(0)\n",
    "                message_args.append(tmp[edge_index[1]])\n",
    "            else:\n",
    "                message_args.append(kwargs[arg])\n",
    "\n",
    "        update_args = [kwargs[arg] for arg in self.update_args]\n",
    "\n",
    "        # Ensure there is at least one argument for the message function\n",
    "        if not message_args:\n",
    "            message_args.append(kwargs['x'])  # Use the node features as a default\n",
    "\n",
    "        out = self.message(*message_args)\n",
    "        out = scatter_(aggr, out, edge_index[0], dim_size=size)\n",
    "        out = self.update(out, *update_args)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        r\"\"\"Constructs messages in analogy to :math:`\\phi_{\\mathbf{\\Theta}}`\n",
    "        for each edge in :math:`(i,j) \\in \\mathcal{E}`.\n",
    "        Can take any argument which was initially passed to :meth:`propagate`.\n",
    "        In addition, features can be lifted to the source node :math:`i` and\n",
    "        target node :math:`j` by appending :obj:`_i` or :obj:`_j` to the\n",
    "        variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.\"\"\"\n",
    "\n",
    "        return x_j\n",
    "\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        r\"\"\"Updates node embeddings in analogy to\n",
    "        :math:`\\gamma_{\\mathbf{\\Theta}}` for each node\n",
    "        :math:`i \\in \\mathcal{V}`.\n",
    "        Takes in the output of aggregation as first argument and any argument\n",
    "        which was initially passed to :meth:`propagate`.\"\"\"\n",
    "\n",
    "        return aggr_out\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Create an instance of the MessagePassing class\n",
    "mp = MessagePassing()\n",
    "\n",
    "# Define the graph with three nodes and two directed edges\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2], [1, 0, 2, 1, 2]], dtype=torch.long)\n",
    "\n",
    "# Create initial node features\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "# Propagate messages and update node embeddings\n",
    "output = mp.propagate('add', edge_index, x=x)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Node Features:\")\n",
    "print(x)\n",
    "print(\"\\nEdge Index:\")\n",
    "print(edge_index)\n",
    "print(\"\\nOutput after Propagation and Update:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2228b64-ce2d-4139-a265-a79e7165203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_to_array(lot):\n",
    "    out = np.array(list(lot[0]))\n",
    "    for i in range(1, len(lot)):\n",
    "        out = np.vstack((out, np.array(list(lot[i]))))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb302b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "lot = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "result = tuple_to_array(lot)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f85b79-34f8-4ad9-9dac-28b158c53d2f",
   "metadata": {},
   "source": [
    "## Step 3: Edge Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_edges(adjs_list):\n",
    "    edges_list = []\n",
    "    for i in range(0, len(adjs_list)):\n",
    "        \n",
    "        adj = adjs_list[i]\n",
    "        adj.eliminate_zeros()        \n",
    "        adj_triu = sp.triu(adj)\n",
    "        adj_tuple = sparse_to_tuple(adj_triu)\n",
    "        edges = adj_tuple[0]\n",
    "        edges_list.append(edges)\n",
    "    return edges_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15385e3e-75f3-4c27-ae43-5e018f8f03ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Create an adjacency matrix\n",
    "\n",
    "temp_matrix = np.random.randint(2, size=(30, 30))\n",
    "temp_adj = np.triu(temp_matrix) + np.triu(temp_matrix, 1).T\n",
    "adj = [sp.csr_matrix(temp_adj)]\n",
    "\n",
    "edge_lists = extract_edges(adj)\n",
    "\n",
    "print(\"edge lists = \", edge_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a606d50-c602-4497-b251-255680c07893",
   "metadata": {},
   "source": [
    "## Step 4: GCN, Temporal Attention, GRU Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921b195-579c-4b12-b4e5-7f3163169ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, act=F.relu, improved=False, bias=False):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.act = act\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones(\n",
    "                (edge_index.size(1), ), dtype=x.dtype, device=x.device)\n",
    "        edge_weight = edge_weight.view(-1)\n",
    "        assert edge_weight.size(0) == edge_index.size(1)\n",
    "\n",
    "        #if your original networks do not have self-loops, uncomment the following:\n",
    "        \n",
    "        # edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        # loop_weight = torch.full(\n",
    "        #     (x.size(0), ),\n",
    "        #     1 if not self.improved else 2,\n",
    "        #     dtype=x.dtype,\n",
    "        #     device=x.device)\n",
    "        # edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=x.size(0))\n",
    "        # print('deg= ',deg)\n",
    "        deg_inv = deg.pow(-0.5)\n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "        # print(\"deg_inv = \",deg_inv)\n",
    "        norm = deg_inv[row] * edge_weight * deg_inv[col]\n",
    "        # print('norm = ',norm)\n",
    "        # print('weigh = ', self.weight)\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        # print('X = ',x)\n",
    "        out = self.propagate('add', edge_index, x=x, norm=norm)\n",
    "        return self.act(out)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa72ee4-989c-4bdb-995a-59a9fe8a4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2, 2], [1, 0, 2, 1, 2]], dtype=torch.long)\n",
    "\n",
    "# Create initial node features\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ae664-3398-45a7-8a51-318a2e2f8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCNConv(2, 2, act=lambda x:x,  improved=False, bias=False)\n",
    "gcn.forward( x, edge_index, edge_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da4fe9-9b1c-4846-8e78-fb94ec18fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Implements an Attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, cuda, nhid):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.nhid = nhid\n",
    "\n",
    "\n",
    "        # Linear transformations for K, Q, V from the same source\n",
    "        self.key = nn.Linear(nhid, nhid)\n",
    "        self.query = nn.Linear(nhid, nhid)\n",
    "        self.value = nn.Linear(nhid, nhid)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.cuda = cuda\n",
    "        self.attention_weights = []\n",
    "\n",
    "    def forward(self, all_h, attention_width=5,mask=None):\n",
    "\n",
    "        temp = [tensor[-1, :, :] for tensor in all_h]\n",
    "        temp = (torch.stack(temp, dim=0))\n",
    "        all_attentions_weights = []\n",
    "        if temp.size(0) >= attention_width:\n",
    "            lb = temp.size(0) - attention_width\n",
    "            if(lb<0):\n",
    "                lb = 0\n",
    "\n",
    "            query = self.query(temp[-1,-1,:].view(1,-1))\n",
    "            # print('shape query =', query.size())\n",
    "            keys = self.key(temp[lb:,-1 ,:])\n",
    "            # print('shape keys =', keys.size())\n",
    "            scores = torch.matmul(query, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.nhid, dtype=torch.float32))\n",
    "                \n",
    "            # Apply mask (if provided)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "            # Apply softmax\n",
    "            attention_weights = F.softmax(scores, dim=1)\n",
    "            all_attentions_weights.append(attention_weights)\n",
    "\n",
    "            # print('att weights size =', attention_weights.size())\n",
    "            \n",
    "            values = self.value( (torch.stack(all_h[lb:], dim=0)))\n",
    "            # print('shape values =', values.size())\n",
    "            \n",
    "            expanded_weights = attention_weights.view(1, attention_weights.size(1), 1, 1, 1)\n",
    "            \n",
    "            weighted_values = values * expanded_weights\n",
    "            # print('wieghted values size = ', weighted_values.size())\n",
    "            \n",
    "            # Sum along the first dimension to get the final weighted sum tensor \n",
    "            weighted_sum = torch.sum(weighted_values, dim=1).squeeze(dim=0)\n",
    "            # print('final tensor size = ', weighted_sum.size())\n",
    "            all_h[-1] = weighted_sum\n",
    "\n",
    "       \n",
    "        return all_h,all_attentions_weights\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac4a6a-9acc-42b2-8bec-00cac24a5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "temp_tensor = [torch.rand(1,3,4) for _ in range(6)]\n",
    "att_layer = AttentionLayer(cuda = False, nhid=4)\n",
    "att_layer.forward(temp_tensor, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83b85c-bfcc-465c-a4df-a3827fb88930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_gru_attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, bias=True):\n",
    "        super(graph_gru_attention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = n_layer\n",
    "        cuda=False\n",
    "        attention_width=5\n",
    "        # gru weights\n",
    "        self.weight_xz = []\n",
    "        self.weight_hz = []\n",
    "        self.weight_xr = []\n",
    "        self.weight_hr = []\n",
    "        self.weight_xh = []\n",
    "        self.weight_hh = []\n",
    "        \n",
    "        self.attention_width = attention_width\n",
    "        self.AttentionLayer = AttentionLayer(cuda,hidden_size)\n",
    "        \n",
    "        for i in range(self.n_layer):\n",
    "            if i==0:\n",
    "                self.weight_xz.append(GCNConv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_hz.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_xr.append(GCNConv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_hr.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_xh.append(GCNConv(input_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_hh.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "            else:\n",
    "                self.weight_xz.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_hz.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_xr.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_hr.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_xh.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "                self.weight_hh.append(GCNConv(hidden_size, hidden_size, act=lambda x:x, bias=bias))\n",
    "    \n",
    "    def forward(self, inp, edgidx, all_h):\n",
    "        h_hat = torch.zeros(all_h[-1].size())\n",
    "        for i in range(self.n_layer):\n",
    "            if i==0:\n",
    "                s_g = torch.sigmoid(self.weight_xz[i](inp, edgidx) + self.weight_hz[i](all_h[-1][i], edgidx))\n",
    "                r_g = torch.sigmoid(self.weight_xr[i](inp, edgidx) + self.weight_hr[i](all_h[-1][i], edgidx))\n",
    "                h_tilde_g = torch.tanh(self.weight_xh[i](inp, edgidx) + self.weight_hh[i](r_g * all_h[-1][i], edgidx))\n",
    "                h_hat[i] = s_g * all_h[-1][i] + (1 - s_g) * h_tilde_g\n",
    "                \n",
    "            else:\n",
    "\n",
    "                s_g = torch.sigmoid(self.weight_xz[i](h_hat[i-1], edgidx) + self.weight_hz[i](all_h[-1][i], edgidx))\n",
    "                r_g = torch.sigmoid(self.weight_xr[i](h_hat[i-1], edgidx) + self.weight_hr[i](all_h[-1][i], edgidx))\n",
    "                h_tilde_g = torch.tanh(self.weight_xh[i](h_hat[i-1], edgidx) + self.weight_hh[i](r_g * all_h[-1][i], edgidx))\n",
    "                h_hat[i] = s_g * all_h[-1][i] + (1 - s_g) * h_tilde_g\n",
    "                \n",
    "        all_h.append(h_hat)\n",
    "        \n",
    "        all_h, attention_weights = self.AttentionLayer.forward(all_h, self.attention_width)\n",
    "        \n",
    "        out = all_h[-1]\n",
    "        return all_h, out, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35383179-1c2c-425a-b1e8-43284f5f0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(nn.Module):\n",
    "    def __init__(self, act=torch.sigmoid, dropout=0.):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        \n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inp = F.dropout(inp, self.dropout, training=self.training)\n",
    "        x = torch.transpose(inp, dim0=0, dim1=1)\n",
    "        x = torch.mm(inp, x)\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de643138-6c74-4433-85ec-e5e292a59d49",
   "metadata": {},
   "source": [
    "## Step 5: Temporal Attention-enhanced Variational Graph Recurrent Neural Network (T-AVRNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18de85-ee65-4def-b6e5-cc2193edf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_AVRNN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, n_layers, eps, bias=False):\n",
    "        super(T_AVRNN, self).__init__()\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.eps = eps\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.phi_x = nn.Sequential(nn.Linear(x_dim, h_dim), nn.ReLU())\n",
    "        self.phi_z = nn.Sequential(nn.Linear(z_dim, h_dim), nn.ReLU())\n",
    "        \n",
    "        self.enc = GCNConv(h_dim + h_dim, h_dim)            \n",
    "        self.enc_mean = GCNConv(h_dim, z_dim, act=lambda x:x)\n",
    "        self.enc_std = GCNConv(h_dim, z_dim, act=F.softplus)\n",
    "        \n",
    "        self.prior = nn.Sequential(nn.Linear(h_dim, h_dim), nn.ReLU())\n",
    "        self.prior_mean = nn.Sequential(nn.Linear(h_dim, z_dim))\n",
    "        self.prior_std = nn.Sequential(nn.Linear(h_dim, z_dim), nn.Softplus())\n",
    "        \n",
    "        self.rnn = graph_gru_attention(h_dim + h_dim, h_dim, n_layers, bias)\n",
    "            \n",
    "  \n",
    "    \n",
    "    def forward(self, x, edge_idx_list, adj_orig_dense_list, hidden_in=None):\n",
    "        assert len(adj_orig_dense_list) == len(edge_idx_list)\n",
    "        \n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "        all_enc_mean, all_enc_std = [], []\n",
    "        all_prior_mean, all_prior_std = [], []\n",
    "        all_dec_t, all_z_t = [], []\n",
    "        all_h = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        if hidden_in is None:\n",
    "            h = Variable(torch.zeros(self.n_layers, x.size(1), self.h_dim))\n",
    "        else:\n",
    "            h = Variable(hidden_in)\n",
    "\n",
    "        all_h.append(h)\n",
    "        for t in range(x.size(0)):\n",
    "            phi_x_t = self.phi_x(x[t])\n",
    "            \n",
    "            #encoder\n",
    "            # print('phi(x) size = ', phi_x_t.size())\n",
    "            # print('h[-1] size =', h[-1].size())\n",
    "            enc_t = self.enc(torch.cat([phi_x_t, h[-1]], 1), edge_idx_list[t])\n",
    "            # print('enc_t size = ',enc_t.size())\n",
    "            enc_mean_t = self.enc_mean(enc_t, edge_idx_list[t])\n",
    "            # print('enc_mean_t size = ',enc_mean_t.size())\n",
    "            enc_std_t = self.enc_std(enc_t, edge_idx_list[t])\n",
    "            # print('enc_std_t size = ',enc_std_t.size())\n",
    "            \n",
    "            #prior\n",
    "            prior_t = self.prior(h[-1])\n",
    "            # print('prior_t size = ',prior_t.size())\n",
    "            prior_mean_t = self.prior_mean(prior_t)\n",
    "            # print('prior_mean_t size = ',prior_mean_t.size())\n",
    "            prior_std_t = self.prior_std(prior_t)\n",
    "            # print('prior_std_t size = ',prior_std_t.size())\n",
    "            \n",
    "            #sampling and reparameterization\n",
    "            z_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "            # print('z_t size = ,',z_t.size())\n",
    "            phi_z_t = self.phi_z(z_t)\n",
    "            # print('phi_z_t size = ',phi_z_t.size())\n",
    "            \n",
    "            #decoder\n",
    "            dec_t = self.dec(z_t)\n",
    "            \n",
    "            #recurrence\n",
    "            all_h, h , attention_weights_t = self.rnn(torch.cat([phi_x_t, phi_z_t], 1), edge_idx_list[t], all_h)\n",
    "            \n",
    "            nnodes = adj_orig_dense_list[t].size()[0]\n",
    "            enc_mean_t_sl = enc_mean_t[0:nnodes, :]\n",
    "            enc_std_t_sl = enc_std_t[0:nnodes, :]\n",
    "            prior_mean_t_sl = prior_mean_t[0:nnodes, :]\n",
    "            prior_std_t_sl = prior_std_t[0:nnodes, :]\n",
    "            dec_t_sl = dec_t[0:nnodes, 0:nnodes]\n",
    "            \n",
    "            #computing losses\n",
    "            kld_loss += self._kld_gauss(enc_mean_t_sl, enc_std_t_sl, prior_mean_t_sl, prior_std_t_sl)\n",
    "            nll_loss += self._nll_bernoulli(dec_t_sl, adj_orig_dense_list[t])\n",
    "            \n",
    "            # all_enc_std.append(enc_std_t_sl)\n",
    "            # all_enc_mean.append(enc_mean_t_sl)\n",
    "            # all_prior_mean.append(prior_mean_t_sl)\n",
    "            # all_prior_std.append(prior_std_t_sl)\n",
    "            # all_dec_t.append(dec_t_sl)\n",
    "            all_z_t.append(z_t)\n",
    "            all_attention_weights.append(attention_weights_t)\n",
    "        return kld_loss, nll_loss, all_z_t , all_attention_weights\n",
    "    \n",
    "    def dec(self, z):\n",
    "        outputs = InnerProductDecoder(act=lambda x:x)(z)\n",
    "        return outputs\n",
    "    \n",
    "    def reset_parameters(self, stdv=1e-1):\n",
    "        for weight in self.parameters():\n",
    "            weight.data.normal_(0, stdv)\n",
    "     \n",
    "    def _init_weights(self, stdv):\n",
    "        pass\n",
    "    \n",
    "    def _reparameterized_sample(self, mean, std):\n",
    "        eps1 = torch.FloatTensor(std.size()).normal_()\n",
    "        eps1 = Variable(eps1)\n",
    "        return eps1.mul(std).add_(mean)\n",
    "    \n",
    "    def _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
    "        num_nodes = mean_1.size()[0]\n",
    "        kld_element =  (2 * torch.log(std_2 + self.eps) - 2 * torch.log(std_1 + self.eps) +\n",
    "                        (torch.pow(std_1 + self.eps ,2) + torch.pow(mean_1 - mean_2, 2)) / \n",
    "                        torch.pow(std_2 + self.eps ,2) - 1)\n",
    "        return (0.5 / num_nodes) * torch.mean(torch.sum(kld_element, dim=1), dim=0)\n",
    "    \n",
    "    def _kld_gauss_zu(self, mean_in, std_in):\n",
    "        num_nodes = mean_in.size()[0]\n",
    "        std_log = torch.log(std_in + self.eps)\n",
    "        kld_element =  torch.mean(torch.sum(1 + 2 * std_log - mean_in.pow(2) -\n",
    "                                            torch.pow(torch.exp(std_log), 2), 1))\n",
    "        return (-0.5 / num_nodes) * kld_element\n",
    "    \n",
    "    def _nll_bernoulli(self, logits, target_adj_dense):\n",
    "        temp_size = target_adj_dense.size()[0]\n",
    "        temp_sum = target_adj_dense.sum()\n",
    "        posw = float(temp_size * temp_size - temp_sum) / temp_sum\n",
    "        norm = temp_size * temp_size / float((temp_size * temp_size - temp_sum) * 2)\n",
    "        nll_loss_mat = F.binary_cross_entropy_with_logits(input=logits\n",
    "                                                          , target=target_adj_dense\n",
    "                                                          , pos_weight=posw\n",
    "                                                          , reduction='none')\n",
    "        nll_loss = -1 * norm * torch.mean(nll_loss_mat, dim=[0,1])\n",
    "        return - nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2564c5-ad8d-4c73-bfa8-b06361fd8534",
   "metadata": {},
   "source": [
    "## Step 6: Creating Temporal Networks and Input Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8cdd11-f4ae-49dc-aeb7-9e7e9a3d055f",
   "metadata": {},
   "source": [
    "#### The _neural_data_ in the following cell should be replaced by your time series data (rows being individual units/neurons/channels) and columns representing the time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923b318-8538-46de-8d7a-67532e14bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Constructing Temporal Networks\n",
    "\n",
    "neural_data = np.random.rand(900,1024)\n",
    "num_timepoints = np.shape(neural_data)[1] \n",
    "\n",
    "# Set parameters for sliding windows\n",
    "window_size = 200 ## equals to 10 seconds\n",
    "step_size = int(window_size/2)  # Choose a step size (overlap between windows)\n",
    "\n",
    "\n",
    "# Create a list to store dynamic functional connectivity networks\n",
    "\n",
    "dynamic_networks = []\n",
    "\n",
    "all_connectivity_matrices = []\n",
    "for start in range(0, num_timepoints - window_size + 1, step_size):\n",
    "    end = start + window_size\n",
    "\n",
    "    # Extract the current window of data\n",
    "    current_window = neural_data[:, start:end]\n",
    "\n",
    "    \n",
    "    # Calculate functional connectivity (correlation in this case)\n",
    "    current_window[np.isnan(current_window)] = 0\n",
    "    connectivity_matrix = np.corrcoef(current_window)\n",
    "    connectivity_matrix[np.isnan(connectivity_matrix)] = 0\n",
    "    \n",
    "\n",
    "\n",
    "    np.fill_diagonal(connectivity_matrix, 1)\n",
    "      \n",
    "    all_connectivity_matrices.append(connectivity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adj_time_list = [sp.csr_matrix(connectivity_mat) for  connectivity_mat in all_connectivity_matrices]\n",
    "\n",
    "num_nodes = np.shape(adj_time_list[0])[0]\n",
    "\n",
    "### Weighted Adjacency Matrix\n",
    "threshold = 0.4\n",
    "edge_weights = [torch.tensor(np.abs(np.where(np.abs(connectivity_matrix.toarray()) < threshold, 0, connectivity_matrix.toarray())), dtype=torch.float32) for connectivity_matrix in adj_time_list]\n",
    "\n",
    "### Binary Adjacency Matrix\n",
    "adj_orig_dense_list = [(weighted_matrix != 0).int() for weighted_matrix in edge_weights] \n",
    "\n",
    "\n",
    "### Adding the History node to the network\n",
    "edge_weights_with_history_node_list = []\n",
    "for weighted_matrix in edge_weights:\n",
    "    # Add a row and column for the new node\n",
    "    new_node_row = torch.zeros(1, num_nodes, dtype=torch.float)\n",
    "    new_node_col = torch.zeros(num_nodes+1, 1, dtype=torch.float)\n",
    "    \n",
    "    # Concatenate the new node row and column to the existing matrix\n",
    "    weighted_matrix = torch.cat([weighted_matrix, new_node_row], dim=0)\n",
    "    weighted_matrix = torch.cat([weighted_matrix, new_node_col], dim=1)\n",
    "    \n",
    "    # Set the connections for the new node\n",
    "    weighted_matrix[-1, :] = 0  # new node is not connected to all other nodes\n",
    "    weighted_matrix[:, -1] = 1  # all other nodes are connected to the new node\n",
    "    weighted_matrix[-1,-1] =1\n",
    "    \n",
    "    # Append the modified matrix to the new list\n",
    "    edge_weights_with_history_node_list.append(weighted_matrix)\n",
    "\n",
    "\n",
    "\n",
    "adj_with_history_node_list = []\n",
    "for adj_matrix in adj_orig_dense_list:\n",
    "    # Add a row and column for the new node\n",
    "    new_node_row = torch.zeros(1, num_nodes, dtype=torch.float)\n",
    "    new_node_col = torch.zeros(num_nodes+1, 1, dtype=torch.float)\n",
    "    \n",
    "    # Concatenate the new node row and column to the existing matrix\n",
    "    adj_matrix = torch.cat([adj_matrix, new_node_row], dim=0)\n",
    "    adj_matrix = torch.cat([adj_matrix, new_node_col], dim=1)\n",
    "    \n",
    "    # Set the connections for the new node\n",
    "    adj_matrix[-1, :] = 0  # new node is not connected to all other nodes\n",
    "    adj_matrix[:, -1] = 1  # all other nodes are connected to the new node\n",
    "    adj_matrix[-1,-1] =1\n",
    "    \n",
    "    # Append the modified matrix to the new list\n",
    "    adj_with_history_node_list.append(adj_matrix)\n",
    "\n",
    "\n",
    "adj_time_list_with_history_node = [sp.csr_matrix(adj_matrix) for adj_matrix in adj_with_history_node_list ]\n",
    "\n",
    "\n",
    "### Node attributes (if you want to use Identity Matrix, use the commented lines)\n",
    "x_in = Variable(torch.stack(edge_weights_with_history_node_list))\n",
    "\n",
    "\n",
    "# seq_len = len(edge_weights_with_history_node_list)\n",
    "# x_in_list = []\n",
    "# for i in range(0, seq_len):\n",
    "#     x_temp = torch.tensor(np.eye(num_nodes+1).astype(np.float32))\n",
    "#     x_in_list.append(torch.tensor(x_temp))\n",
    "\n",
    "# x_in = Variable(torch.stack(x_in_list))\n",
    "\n",
    "\n",
    "### creating edge list\n",
    "all_edges = extract_edges(adj_time_list_with_history_node)\n",
    "edge_idx_list = []\n",
    "\n",
    "for i in range(len(all_edges)):\n",
    "    edge_idx_list.append(torch.tensor(np.transpose(all_edges[i]), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916293c-5e56-4cb1-a8ef-a4b6e59afdc1",
   "metadata": {},
   "source": [
    "## Step 7: Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff69478-b97a-422d-aed0-bd75e59cc030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "h_dim = 32\n",
    "z_dim = 16\n",
    "n_layers = 1\n",
    "clip = 10\n",
    "learning_rate = 1e-2\n",
    "num_nodes = np.shape(adj_time_list[0])[0]\n",
    "eps = 1e-10\n",
    "x_dim = num_nodes + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65916f0b-ee0e-4b4e-abfb-49269f287c85",
   "metadata": {},
   "source": [
    "## Step 8: Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd076d7-fe5e-4b46-9d07-99d0b0dc8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T_AVRNN(x_dim, h_dim, z_dim, n_layers, eps, bias=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e7465-22f1-45b2-832a-720006fc6c02",
   "metadata": {},
   "source": [
    "## Step 9: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c739c8-1bb5-426a-bd5f-d0f3fb252a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_loss = 0\n",
    "for k in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    kld_loss, nll_loss, all_z, all_att_w = model(x_in, edge_idx_list, adj_with_history_node_list)\n",
    "    loss = kld_loss + nll_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    print('Epoch:', k)\n",
    "    print('KLD Loss:', kld_loss.mean().item())\n",
    "    print('NLL Loss:', nll_loss.mean().item())\n",
    "    print('Total Loss:', loss.mean().item())\n",
    "    print('-------------------------')\n",
    "    diff = np.abs(last_loss - loss.mean().item())\n",
    "    last_loss = loss.mean().item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e53a77-04ac-460f-bc12-83d93365a928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
